"""
M√©tricas personalizadas para evaluaci√≥n de modelos ML

Incluye m√©tricas para:
- Regresi√≥n (ETA prediction)
- Clasificaci√≥n (Driver behavior)
- Anomaly detection
"""

import numpy as np
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report,
    roc_auc_score, roc_curve
)


# ============================================================================
# M√âTRICAS DE REGRESI√ìN (para predicci√≥n de ETA, distancias, etc.)
# ============================================================================

def rmse(y_true, y_pred):
    """
    Root Mean Squared Error (RMSE)
    Penaliza m√°s los errores grandes
    
    Args:
        y_true: Valores reales
        y_pred: Valores predichos
    
    Returns:
        RMSE score
    """
    return np.sqrt(mean_squared_error(y_true, y_pred))


def mae(y_true, y_pred):
    """
    Mean Absolute Error (MAE)
    Error absoluto promedio
    
    Args:
        y_true: Valores reales
        y_pred: Valores predichos
    
    Returns:
        MAE score
    """
    return mean_absolute_error(y_true, y_pred)


def mape(y_true, y_pred):
    """
    Mean Absolute Percentage Error (MAPE)
    Error porcentual promedio
    
    Args:
        y_true: Valores reales
        y_pred: Valores predichos
    
    Returns:
        MAPE score (en porcentaje)
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    # Evitar divisi√≥n por cero
    mask = y_true != 0
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100


def evaluate_regression_model(y_true, y_pred, model_name='Model'):
    """
    Eval√∫a un modelo de regresi√≥n con m√∫ltiples m√©tricas
    
    Args:
        y_true: Valores reales
        y_pred: Valores predichos
        model_name: Nombre del modelo
    
    Returns:
        Dict con todas las m√©tricas
    """
    metrics = {
        'RMSE': rmse(y_true, y_pred),
        'MAE': mae(y_true, y_pred),
        'MAPE': mape(y_true, y_pred),
        'R2': r2_score(y_true, y_pred)
    }
    
    print(f"\n{'='*50}")
    print(f"üìä M√©tricas de Regresi√≥n: {model_name}")
    print(f"{'='*50}")
    print(f"RMSE:  {metrics['RMSE']:.4f}")
    print(f"MAE:   {metrics['MAE']:.4f}")
    print(f"MAPE:  {metrics['MAPE']:.2f}%")
    print(f"R¬≤:    {metrics['R2']:.4f}")
    print(f"{'='*50}\n")
    
    return metrics


# ============================================================================
# M√âTRICAS DE CLASIFICACI√ìN (para clasificaci√≥n de conductores)
# ============================================================================

def evaluate_classification_model(y_true, y_pred, y_pred_proba=None, 
                                  class_names=None, model_name='Model'):
    """
    Eval√∫a un modelo de clasificaci√≥n con m√∫ltiples m√©tricas
    
    Args:
        y_true: Etiquetas reales
        y_pred: Etiquetas predichas
        y_pred_proba: Probabilidades predichas (opcional, para ROC-AUC)
        class_names: Nombres de las clases (opcional)
        model_name: Nombre del modelo
    
    Returns:
        Dict con todas las m√©tricas
    """
    # Determinar si es binario o multiclase
    n_classes = len(np.unique(y_true))
    is_binary = n_classes == 2
    
    # Calcular m√©tricas b√°sicas
    accuracy = accuracy_score(y_true, y_pred)
    
    if is_binary:
        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
    else:
        # Para multiclase, usar promedio weighted
        precision = precision_score(y_true, y_pred, average='weighted')
        recall = recall_score(y_true, y_pred, average='weighted')
        f1 = f1_score(y_true, y_pred, average='weighted')
    
    metrics = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1
    }
    
    # Calcular ROC-AUC si se proporcionaron probabilidades
    if y_pred_proba is not None:
        if is_binary:
            # Para binario, usar columna de clase positiva
            if y_pred_proba.ndim > 1:
                y_pred_proba = y_pred_proba[:, 1]
            metrics['ROC-AUC'] = roc_auc_score(y_true, y_pred_proba)
        else:
            # Para multiclase, usar one-vs-rest
            try:
                metrics['ROC-AUC'] = roc_auc_score(y_true, y_pred_proba, 
                                                   multi_class='ovr', average='weighted')
            except:
                pass
    
    # Imprimir resultados
    print(f"\n{'='*50}")
    print(f"üìä M√©tricas de Clasificaci√≥n: {model_name}")
    print(f"{'='*50}")
    print(f"Accuracy:  {metrics['Accuracy']:.4f}")
    print(f"Precision: {metrics['Precision']:.4f}")
    print(f"Recall:    {metrics['Recall']:.4f}")
    print(f"F1-Score:  {metrics['F1-Score']:.4f}")
    if 'ROC-AUC' in metrics:
        print(f"ROC-AUC:   {metrics['ROC-AUC']:.4f}")
    
    # Matriz de confusi√≥n
    cm = confusion_matrix(y_true, y_pred)
    print(f"\nüìä Matriz de Confusi√≥n:")
    print(cm)
    
    # Reporte detallado
    print(f"\nüìã Reporte de Clasificaci√≥n:")
    print(classification_report(y_true, y_pred, target_names=class_names))
    
    print(f"{'='*50}\n")
    
    return metrics


# ============================================================================
# M√âTRICAS DE DETECCI√ìN DE ANOMAL√çAS
# ============================================================================

def evaluate_anomaly_detection(y_true, y_pred, y_scores=None, model_name='Model'):
    """
    Eval√∫a un modelo de detecci√≥n de anomal√≠as
    
    Args:
        y_true: Etiquetas reales (1=anomal√≠a, 0=normal)
        y_pred: Etiquetas predichas (1=anomal√≠a, 0=normal)
        y_scores: Scores de anomal√≠a (opcional)
        model_name: Nombre del modelo
    
    Returns:
        Dict con todas las m√©tricas
    """
    # Convertir -1 a 1 si es necesario (algunos modelos usan -1 para anomal√≠a)
    y_pred_binary = np.where(y_pred == -1, 1, y_pred)
    
    # Calcular m√©tricas
    cm = confusion_matrix(y_true, y_pred_binary)
    
    # True Negatives, False Positives, False Negatives, True Positives
    tn, fp, fn, tp = cm.ravel()
    
    accuracy = accuracy_score(y_true, y_pred_binary)
    precision = precision_score(y_true, y_pred_binary, zero_division=0)
    recall = recall_score(y_true, y_pred_binary, zero_division=0)
    f1 = f1_score(y_true, y_pred_binary, zero_division=0)
    
    # Tasa de falsos positivos
    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
    
    metrics = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'False Positive Rate': false_positive_rate,
        'True Positives': tp,
        'False Positives': fp,
        'True Negatives': tn,
        'False Negatives': fn
    }
    
    # Calcular ROC-AUC si se proporcionaron scores
    if y_scores is not None:
        try:
            metrics['ROC-AUC'] = roc_auc_score(y_true, y_scores)
        except:
            pass
    
    # Imprimir resultados
    print(f"\n{'='*50}")
    print(f"üîç M√©tricas de Detecci√≥n de Anomal√≠as: {model_name}")
    print(f"{'='*50}")
    print(f"Accuracy:  {metrics['Accuracy']:.4f}")
    print(f"Precision: {metrics['Precision']:.4f}")
    print(f"Recall:    {metrics['Recall']:.4f}")
    print(f"F1-Score:  {metrics['F1-Score']:.4f}")
    print(f"FPR:       {metrics['False Positive Rate']:.4f}")
    if 'ROC-AUC' in metrics:
        print(f"ROC-AUC:   {metrics['ROC-AUC']:.4f}")
    
    print(f"\nüìä Matriz de Confusi√≥n:")
    print(f"                 Predicho")
    print(f"              Normal  Anomal√≠a")
    print(f"Real Normal    {tn:5d}    {fp:5d}")
    print(f"     Anomal√≠a  {fn:5d}    {tp:5d}")
    
    print(f"{'='*50}\n")
    
    return metrics


# ============================================================================
# M√âTRICAS PERSONALIZADAS PARA GPS
# ============================================================================

def calculate_eta_accuracy(y_true_eta, y_pred_eta, tolerance_minutes=5):
    """
    Calcula qu√© porcentaje de predicciones de ETA est√°n dentro de un margen de error
    
    Args:
        y_true_eta: Tiempos reales (en minutos)
        y_pred_eta: Tiempos predichos (en minutos)
        tolerance_minutes: Margen de error aceptable (default 5 min)
    
    Returns:
        Porcentaje de predicciones dentro del margen
    """
    errors = np.abs(y_true_eta - y_pred_eta)
    within_tolerance = (errors <= tolerance_minutes).sum()
    accuracy = (within_tolerance / len(y_true_eta)) * 100
    
    return accuracy


def calculate_route_efficiency_score(predicted_distance, actual_distance):
    """
    Calcula qu√© tan eficiente fue la ruta predicha vs la real
    
    Args:
        predicted_distance: Distancia predicha (km)
        actual_distance: Distancia real recorrida (km)
    
    Returns:
        Score de eficiencia (100 = perfecto, <100 = subestim√≥, >100 = sobreestim√≥)
    """
    if actual_distance == 0:
        return 0
    
    efficiency_score = (predicted_distance / actual_distance) * 100
    
    return efficiency_score
